{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\verma\\PycharmProjects\\pyspark\\data\n"
     ]
    }
   ],
   "source": [
    "cd data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Acer\n",
      " Volume Serial Number is 3498-618E\n",
      "\n",
      " Directory of C:\\Users\\verma\\PycharmProjects\\pyspark\\data\n",
      "\n",
      "19-01-2020  21:29    <DIR>          .\n",
      "19-01-2020  21:29    <DIR>          ..\n",
      "19-01-2020  21:29             2,176 airline-passengers.csv\n",
      "19-01-2020  21:29         4,655,560 bank-full.csv\n",
      "19-01-2020  21:29         4,432,265 Bible.txt\n",
      "19-01-2020  21:29            13,516 bitcoin.csv\n",
      "19-01-2020  21:29    <DIR>          cdc\n",
      "19-01-2020  21:29    <DIR>          Churn prediction\n",
      "19-01-2020  21:29         1,219,953 CleanCreditScoring.csv\n",
      "19-01-2020  21:29         1,591,255 Combined_Cycle_Power_Plant.csv\n",
      "19-01-2020  21:29    <DIR>          country\n",
      "19-01-2020  21:29           134,639 credit-default.csv\n",
      "19-01-2020  21:29    <DIR>          datasets\n",
      "19-01-2020  21:29             4,928 Davis.csv\n",
      "19-01-2020  21:29         2,826,084 diamonds.csv\n",
      "19-01-2020  21:29         1,964,610 DigitalBreathTestData2013.zip\n",
      "19-01-2020  21:29            19,130 gdp.csv\n",
      "19-01-2020  21:29           510,678 groceries.csv\n",
      "19-01-2020  21:29    <DIR>          handsigns\n",
      "19-01-2020  21:29        25,905,702 imdb-comments.json.zip\n",
      "19-01-2020  21:29    <DIR>          imdb-tfidf\n",
      "19-01-2020  21:29            55,631 insurance.csv\n",
      "19-01-2020  21:29             5,258 iris.csv\n",
      "19-01-2020  21:29            57,459 istanbul-stock.csv\n",
      "19-01-2020  21:29    <DIR>          kaggle-houseprice\n",
      "19-01-2020  21:29    <DIR>          mnist\n",
      "19-01-2020  21:29               226 mobile-sales-data.csv\n",
      "19-01-2020  21:29    <DIR>          movielens\n",
      "19-01-2020  21:29         5,160,960 movie-lens.db\n",
      "19-01-2020  21:29         7,523,081 Online-Retail.csv.zip\n",
      "19-01-2020  21:29                 8 README.md\n",
      "19-01-2020  21:29    <DIR>          retail\n",
      "19-01-2020  21:29           474,568 sms.csv\n",
      "19-01-2020  21:29         2,661,137 snsdata.csv\n",
      "19-01-2020  21:29            54,758 SP500.csv\n",
      "19-01-2020  21:29             2,431 startups.csv\n",
      "19-01-2020  21:29         8,579,981 stocks.csv.zip\n",
      "19-01-2020  21:29           128,898 stocks.small.csv\n",
      "19-01-2020  21:29            57,714 tcs-stock.csv\n",
      "19-01-2020  21:29            61,194 Titanic.csv\n",
      "19-01-2020  21:29           311,031 tweets.small.json\n",
      "19-01-2020  21:29         9,041,034 vehicle-movement.gz\n",
      "              30 File(s)     77,455,865 bytes\n",
      "              12 Dir(s)  18,701,012,992 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://Raja:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.0.0-preview2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2881bf49080>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\").option(\"header\",\"true\")\\\n",
    ".option(\"inferSchema\",\"true\").load(\"data\\Combined_Cycle_Power_Plant.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+\n",
      "|   AT|    V|     AP|   RH|    EP|\n",
      "+-----+-----+-------+-----+------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|\n",
      "|25.18|62.96|1020.04|59.08|444.37|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|\n",
      "|20.86|57.32|1010.24|76.64|446.48|\n",
      "|10.82| 37.5|1009.23|96.62| 473.9|\n",
      "|26.27|59.44|1012.23|58.77|443.67|\n",
      "|15.89|43.96|1014.02|75.24|467.35|\n",
      "| 9.48|44.71|1019.12|66.43|478.42|\n",
      "|14.64| 45.0|1021.78|41.25|475.98|\n",
      "|11.74|43.56|1015.14|70.72| 477.5|\n",
      "|17.99|43.72|1008.64|75.04|453.02|\n",
      "|20.14|46.93|1014.66|64.22|453.99|\n",
      "|24.34| 73.5|1011.31|84.15|440.29|\n",
      "|25.71|58.59|1012.77|61.83|451.28|\n",
      "|26.19|69.34|1009.48|87.59|433.99|\n",
      "|21.42|43.79|1015.76|43.08|462.19|\n",
      "|18.21| 45.0|1022.86|48.84|467.54|\n",
      "|11.04|41.74| 1022.6|77.51| 477.2|\n",
      "|14.45|52.75|1023.97|63.59|459.85|\n",
      "|13.97|38.47|1015.15|55.28| 464.3|\n",
      "+-----+-----+-------+-----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[AT: double, V: double, AP: double, RH: double, EP: double]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Convert Spark Dataframe to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df.limit(10).toPandas().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+---------------------------+\n",
      "|AT   |V    |AP     |RH   |EP    |features                   |\n",
      "+-----+-----+-------+-----+------+---------------------------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024.07,73.17]|\n",
      "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020.04,59.08]|\n",
      "|5.11 |39.4 |1012.16|92.14|488.56|[5.11,39.4,1012.16,92.14]  |\n",
      "|20.86|57.32|1010.24|76.64|446.48|[20.86,57.32,1010.24,76.64]|\n",
      "|10.82|37.5 |1009.23|96.62|473.9 |[10.82,37.5,1009.23,96.62] |\n",
      "|26.27|59.44|1012.23|58.77|443.67|[26.27,59.44,1012.23,58.77]|\n",
      "|15.89|43.96|1014.02|75.24|467.35|[15.89,43.96,1014.02,75.24]|\n",
      "|9.48 |44.71|1019.12|66.43|478.42|[9.48,44.71,1019.12,66.43] |\n",
      "|14.64|45.0 |1021.78|41.25|475.98|[14.64,45.0,1021.78,41.25] |\n",
      "|11.74|43.56|1015.14|70.72|477.5 |[11.74,43.56,1015.14,70.72]|\n",
      "+-----+-----+-------+-----+------+---------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = VectorAssembler()\n",
    "vectorizer.setInputCols([\"AT\", \"V\", \"AP\", \"RH\"])\n",
    "vectorizer.setOutputCol(\"features\")\n",
    "\n",
    "df_vect = vectorizer.transform(df)\n",
    "df_vect.show(10, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "handleInvalid: How to handle invalid data (NULL and NaN values). Options are 'skip' (filter out rows with invalid data), 'error' (throw an error), or 'keep' (return relevant number of NaN in the output). Column lengths are taken from the size of ML Attribute Group, which can be set using `VectorSizeHint` in a pipeline before `VectorAssembler`. Column lengths can also be inferred from first rows of the data since it is safe to do so but only in case of 'error' or 'skip'). (default: error)\n",
      "inputCols: input column names. (current: ['AT', 'V', 'AP', 'RH'])\n",
      "outputCol: output column name. (default: VectorAssembler_61b67fc01030__output, current: features)\n"
     ]
    }
   ],
   "source": [
    "print(vectorizer.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n",
      "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n",
      "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber (default: 1.35)\n",
      "featuresCol: features column name. (default: features)\n",
      "fitIntercept: whether to fit an intercept term. (default: True)\n",
      "labelCol: label column name. (default: label)\n",
      "loss: The loss function to be optimized. Supported options: squaredError, huber. (default: squaredError)\n",
      "maxIter: max number of iterations (>= 0). (default: 100)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "regParam: regularization parameter (>= 0). (default: 0.0)\n",
      "solver: The solver algorithm for optimization. Supported options: auto, normal, l-bfgs. (default: auto)\n",
      "standardization: whether to standardize the training features before fitting the model. (default: True)\n",
      "tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "print(lr.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr.setLabelCol(\"EP\")\n",
    "lr.setFeaturesCol(\"features\")\n",
    "model = lr.fit(df_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.regression.LinearRegressionModel"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2: 0.9286835997167648\n",
      "Intercept:  454.5637357984046 Coefficients [-1.9773160434618613,-0.23402845649906473,0.06212776009866186,-0.15801655439825457]\n"
     ]
    }
   ],
   "source": [
    "print(\"R2:\", model.summary.r2)\n",
    "print(\"Intercept: \", model.intercept, \"Coefficients\", model.coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "|   AT|    V|     AP|   RH|    EP|            features|        prediction|\n",
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024...| 467.2711634437306|\n",
      "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020...|444.07766858004396|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|[5.11,39.4,1012.1...|483.56251796945776|\n",
      "|20.86|57.32|1010.24|76.64|446.48|[20.86,57.32,1010...| 450.5559716382537|\n",
      "|10.82| 37.5|1009.23|96.62| 473.9|[10.82,37.5,1009....| 471.8267489278455|\n",
      "|26.27|59.44|1012.23|58.77|443.67|[26.27,59.44,1012...| 442.3099415850402|\n",
      "|15.89|43.96|1014.02|75.24|467.35|[15.89,43.96,1014...|463.96591866241715|\n",
      "| 9.48|44.71|1019.12|66.43|478.42|[9.48,44.71,1019....| 478.1739705793852|\n",
      "|14.64| 45.0|1021.78|41.25|475.98|[14.64,45.0,1021....|472.04726822434776|\n",
      "|11.74|43.56|1015.14|70.72| 477.5|[11.74,43.56,1015...| 473.0492095425741|\n",
      "|17.99|43.72|1008.64|75.04|453.02|[17.99,43.72,1008...| 459.5670777622559|\n",
      "|20.14|46.93|1014.66|64.22|453.99|[20.14,46.93,1014...|456.64836515783395|\n",
      "|24.34| 73.5|1011.31|84.15|440.29|[24.34,73.5,1011....| 438.7681037606262|\n",
      "|25.71|58.59|1012.77|61.83|451.28|[25.71,58.59,1012...| 443.1661810913976|\n",
      "|26.19|69.34|1009.48|87.59|433.99|[26.19,69.34,1009...| 435.4263567111474|\n",
      "|21.42|43.79|1015.76|43.08|462.19|[21.42,43.79,1015...| 458.2610604716974|\n",
      "|18.21| 45.0|1022.86|48.84|467.54|[18.21,45.0,1022....|463.85600228221267|\n",
      "|11.04|41.74| 1022.6|77.51| 477.2|[11.04,41.74,1022...| 474.2498032497976|\n",
      "|14.45|52.75|1023.97|63.59|459.85|[14.45,52.75,1023...| 467.2152077040968|\n",
      "|13.97|38.47|1015.15|55.28| 464.3|[13.97,38.47,1015...|472.27139648674444|\n",
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pred = model.transform(df_vect)\n",
    "df_pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labelCol: label column name. (default: label)\n",
      "metricName: metric name in evaluation - one of:\n",
      "                       rmse - root mean squared error (default)\n",
      "                       mse - mean squared error\n",
      "                       r2 - r^2 metric\n",
      "                       mae - mean absolute error\n",
      "                       var - explained variance. (default: rmse)\n",
      "predictionCol: prediction column name. (default: prediction)\n",
      "throughOrigin: whether the regression is through the origin. (default: False)\n",
      "weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "evaluator = RegressionEvaluator()\n",
    "print(evaluator.explainParams())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557525128298466"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(labelCol = \"EP\", \n",
    "                                predictionCol = \"prediction\", \n",
    "                                metricName = \"rmse\")\n",
    "evaluator.evaluate(df_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.pipeline import Pipeline, PipelineModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stages: a list of pipeline stages (undefined)\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline()\n",
    "print(pipeline.explainParams())\n",
    "pipeline.setStages([vectorizer, lr])\n",
    "pipelineModel = pipeline.fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_61b67fc01030, LinearRegression_40ba8956a7e9]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.9773, -0.234, 0.0621, -0.158])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model = pipelineModel.stages[1]\n",
    "lr_model .coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "|   AT|    V|     AP|   RH|    EP|            features|        prediction|\n",
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024...| 467.2711634437306|\n",
      "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020...|444.07766858004396|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|[5.11,39.4,1012.1...|483.56251796945776|\n",
      "|20.86|57.32|1010.24|76.64|446.48|[20.86,57.32,1010...| 450.5559716382537|\n",
      "|10.82| 37.5|1009.23|96.62| 473.9|[10.82,37.5,1009....| 471.8267489278455|\n",
      "|26.27|59.44|1012.23|58.77|443.67|[26.27,59.44,1012...| 442.3099415850402|\n",
      "|15.89|43.96|1014.02|75.24|467.35|[15.89,43.96,1014...|463.96591866241715|\n",
      "| 9.48|44.71|1019.12|66.43|478.42|[9.48,44.71,1019....| 478.1739705793852|\n",
      "|14.64| 45.0|1021.78|41.25|475.98|[14.64,45.0,1021....|472.04726822434776|\n",
      "|11.74|43.56|1015.14|70.72| 477.5|[11.74,43.56,1015...| 473.0492095425741|\n",
      "|17.99|43.72|1008.64|75.04|453.02|[17.99,43.72,1008...| 459.5670777622559|\n",
      "|20.14|46.93|1014.66|64.22|453.99|[20.14,46.93,1014...|456.64836515783395|\n",
      "|24.34| 73.5|1011.31|84.15|440.29|[24.34,73.5,1011....| 438.7681037606262|\n",
      "|25.71|58.59|1012.77|61.83|451.28|[25.71,58.59,1012...| 443.1661810913976|\n",
      "|26.19|69.34|1009.48|87.59|433.99|[26.19,69.34,1009...| 435.4263567111474|\n",
      "|21.42|43.79|1015.76|43.08|462.19|[21.42,43.79,1015...| 458.2610604716974|\n",
      "|18.21| 45.0|1022.86|48.84|467.54|[18.21,45.0,1022....|463.85600228221267|\n",
      "|11.04|41.74| 1022.6|77.51| 477.2|[11.04,41.74,1022...| 474.2498032497976|\n",
      "|14.45|52.75|1023.97|63.59|459.85|[14.45,52.75,1023...| 467.2152077040968|\n",
      "|13.97|38.47|1015.15|55.28| 464.3|[13.97,38.47,1015...|472.27139648674444|\n",
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipelineModel.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.557525128298466"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluator.evaluate(pipelineModel.transform(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Save the pipeline to disk to persist the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pipelineModel.save(\"/tmp/lr-pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Invalid switch - /tmp/lr-pipeline\n"
     ]
    }
   ],
   "source": [
    "!tree /tmp/lr-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-1.9773, -0.234, 0.0621, -0.158])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_model = PipelineModel.load(\"/tmp/lr-pipeline\")\n",
    "saved_model.stages[1].coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "|   AT|    V|     AP|   RH|    EP|            features|        prediction|\n",
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "|14.96|41.76|1024.07|73.17|463.26|[14.96,41.76,1024...| 467.2711634437306|\n",
      "|25.18|62.96|1020.04|59.08|444.37|[25.18,62.96,1020...|444.07766858004396|\n",
      "| 5.11| 39.4|1012.16|92.14|488.56|[5.11,39.4,1012.1...|483.56251796945776|\n",
      "|20.86|57.32|1010.24|76.64|446.48|[20.86,57.32,1010...| 450.5559716382537|\n",
      "|10.82| 37.5|1009.23|96.62| 473.9|[10.82,37.5,1009....| 471.8267489278455|\n",
      "|26.27|59.44|1012.23|58.77|443.67|[26.27,59.44,1012...| 442.3099415850402|\n",
      "|15.89|43.96|1014.02|75.24|467.35|[15.89,43.96,1014...|463.96591866241715|\n",
      "| 9.48|44.71|1019.12|66.43|478.42|[9.48,44.71,1019....| 478.1739705793852|\n",
      "|14.64| 45.0|1021.78|41.25|475.98|[14.64,45.0,1021....|472.04726822434776|\n",
      "|11.74|43.56|1015.14|70.72| 477.5|[11.74,43.56,1015...| 473.0492095425741|\n",
      "|17.99|43.72|1008.64|75.04|453.02|[17.99,43.72,1008...| 459.5670777622559|\n",
      "|20.14|46.93|1014.66|64.22|453.99|[20.14,46.93,1014...|456.64836515783395|\n",
      "|24.34| 73.5|1011.31|84.15|440.29|[24.34,73.5,1011....| 438.7681037606262|\n",
      "|25.71|58.59|1012.77|61.83|451.28|[25.71,58.59,1012...| 443.1661810913976|\n",
      "|26.19|69.34|1009.48|87.59|433.99|[26.19,69.34,1009...| 435.4263567111474|\n",
      "|21.42|43.79|1015.76|43.08|462.19|[21.42,43.79,1015...| 458.2610604716974|\n",
      "|18.21| 45.0|1022.86|48.84|467.54|[18.21,45.0,1022....|463.85600228221267|\n",
      "|11.04|41.74| 1022.6|77.51| 477.2|[11.04,41.74,1022...| 474.2498032497976|\n",
      "|14.45|52.75|1023.97|63.59|459.85|[14.45,52.75,1023...| 467.2152077040968|\n",
      "|13.97|38.47|1015.15|55.28| 464.3|[13.97,38.47,1015...|472.27139648674444|\n",
      "+-----+-----+-------+-----+------+--------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "saved_model.transform(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit(weights=[0.7, 0.3], seed = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.636179253116803"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(df_train)\n",
    "evaluator.evaluate(pipelineModel.transform(df_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = ParamGridBuilder()\\\n",
    "    .addGrid(lr.regParam, [0.1, 0.01]) \\\n",
    "    .addGrid(lr.fitIntercept, [False, True])\\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\\\n",
    "    .build()\n",
    "\n",
    "# In this case the estimator is simply the linear regression.\n",
    "# A TrainValidationSplit requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n",
    "tvs = TrainValidationSplit(estimator=lr,\n",
    "                           estimatorParamMaps=paramGrid,\n",
    "                           evaluator=evaluator,\n",
    "                           trainRatio=0.8)\n",
    "\n",
    "tuned_model = tvs.fit(vectorizer.transform(df_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(LinearRegressionModel: uid=LinearRegression_40ba8956a7e9, numFeatures=4,\n",
       " [5.034026952487715,\n",
       "  5.410220820200018,\n",
       "  5.6267831123193375,\n",
       "  4.548293115745815,\n",
       "  4.550488797562064,\n",
       "  4.554343826782874,\n",
       "  5.032860454074086,\n",
       "  5.202237280212853,\n",
       "  5.221187417360323,\n",
       "  4.5449959665589,\n",
       "  4.545244536358477,\n",
       "  4.545511107837333])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_model.bestModel, tuned_model.validationMetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
